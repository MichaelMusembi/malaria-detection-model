{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malaria Cell Detection with Deep Learning\n",
    "\n",
    "Malaria remains one of the most severe public health challenges worldwide, particularly in sub-Saharan Africa. Caused by the *Plasmodium* parasite and transmitted through the bites of infected Anopheles mosquitoes, malaria leads to hundreds of thousands of deaths annually, with children under five being the most vulnerable.\n",
    "\n",
    "Early and accurate diagnosis is critical for effective treatment and reducing mortality. Traditional methods, such as microscopic examination of blood smears, are time-consuming, require trained personnel, and are prone to human error.\n",
    "\n",
    "Automated detection of malaria-infected cells using digital microscopy images and deep learning offers a scalable and fast alternative. Convolutional Neural Networks (CNNs) and transfer learning models can learn to identify parasitized cells from healthy ones by extracting complex features from high-dimensional image data, enabling accurate and rapid diagnosis.\n",
    "\n",
    "## Key Impacts:\n",
    "\n",
    "- **Medical & Public Health:** Reduces diagnostic errors, accelerates treatment initiation, and contributes to malaria control programs.\n",
    "- **Efficiency:** Automates the labor-intensive process of manual blood smear analysis, allowing laboratory staff to focus on critical tasks.\n",
    "- **Scalability:** Can be deployed in resource-limited settings where trained microscopists are scarce, improving healthcare accessibility.\n",
    "\n",
    "This project explores the use of classical machine learning models, a custom CNN, and transfer learning with VGG16 to build an automated pipeline for malaria cell detection. The goal is to create a robust, accurate, and generalizable model capable of distinguishing parasitized cells from healthy ones in digital blood smear images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Setup and Download\n",
    "\n",
    "In this step, we set up the environment, import the necessary libraries, and download the dataset using the **KaggleHub API**.  \n",
    "The dataset contains images of **Parasitized** and **Uninfected** cells, which will later be used for binary classification.  \n",
    "\n",
    "We will also verify that the dataset has been downloaded and explore the structure to confirm that both classes exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: We started by setting up, downloading and verifying the dataset\n",
    "\n",
    "# Installation of kagglehub\n",
    "!pip install -q kagglehub --upgrade\n",
    "\n",
    "# Imports and reproducibility\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# For this we downloaded the dataset from Kagglehub\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Requesting dataset download... (this may take a minute)\")\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"iarunava/cell-images-for-detecting-malaria\")\n",
    "print(\"kagglehub returned path:\", dataset_path)\n",
    "\n",
    "# Set base_dir \n",
    "base_dir = Path(dataset_path)\n",
    "if base_dir.is_file() and str(base_dir).lower().endswith('.zip'):\n",
    "    print(\"Downloaded a zip file. Extracting...\")\n",
    "    import zipfile\n",
    "    extract_to = Path(\"/kaggle/working/cell_images\")\n",
    "    with zipfile.ZipFile(str(base_dir), 'r') as z:\n",
    "        z.extractall(extract_to)\n",
    "    base_dir = extract_to\n",
    "    print(\"Extracted to:\", base_dir)\n",
    "\n",
    "if (base_dir / \"cell_images\").exists():\n",
    "    base_dir = base_dir / \"cell_images\"\n",
    "\n",
    "parasitized_dir = base_dir / 'Parasitized'\n",
    "uninfected_dir = base_dir / 'Uninfected'\n",
    "\n",
    "# Conducted Safety Checks for the Dataset\n",
    "assert parasitized_dir.exists(), f\"Parasitized folder not found at {parasitized_dir}\"\n",
    "assert uninfected_dir.exists(), f\"Uninfected folder not found at {uninfected_dir}\"\n",
    "\n",
    "parasitized_count = len(list(parasitized_dir.glob('*.png')))\n",
    "uninfected_count = len(list(uninfected_dir.glob('*.png')))\n",
    "\n",
    "print(f\"Parasitized samples: {parasitized_count}\")\n",
    "print(f\"Uninfected samples:  {uninfected_count}\")\n",
    "print(f\"Base data directory: {base_dir}\")\n",
    "\n",
    "# Printed a few example file names\n",
    "print(\"\\nExample Parasitized files (first 3):\")\n",
    "for p in list(parasitized_dir.glob('*.png'))[:3]:\n",
    "    print(\" \", p.name)\n",
    "print(\"\\nExample Uninfected files (first 3):\")\n",
    "for p in list(uninfected_dir.glob('*.png'))[:3]:\n",
    "    print(\" \", p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Visualization and Class Distribution\n",
    "\n",
    "Before building any models, it's essential to visualize the dataset and confirm that both classes are well represented.  \n",
    "In this step, we will:\n",
    "- Plot a few random images from each class (*Parasitized* and *Uninfected*).\n",
    "- Display a simple bar chart showing the number of images in each class.\n",
    "\n",
    "This helps us understand the dataset and verify that the data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize Images and Class Counts\n",
    "\n",
    "import random\n",
    "\n",
    "# Get random samples from each class\n",
    "parasitized_samples = random.sample(list(parasitized_dir.glob('*.png')), 4)\n",
    "uninfected_samples = random.sample(list(uninfected_dir.glob('*.png')), 4)\n",
    "\n",
    "# Display random images from both classes\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('Sample Images from Each Class', fontsize=16)\n",
    "\n",
    "for i, img_path in enumerate(parasitized_samples):\n",
    "    img = mpimg.imread(img_path)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title('Parasitized')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, img_path in enumerate(uninfected_samples):\n",
    "    img = mpimg.imread(img_path)\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].set_title('Uninfected')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(['Parasitized', 'Uninfected'], [parasitized_count, uninfected_count], color=['#e74c3c', '#2ecc71'])\n",
    "plt.title('Class Distribution')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing and Splitting\n",
    "\n",
    "To train our models effectively, we need to preprocess the images and divide the dataset into **training** and **validation** sets.\n",
    "\n",
    "In this step:\n",
    "- We'll use `ImageDataGenerator` from Keras to rescale pixel values and apply **data augmentation** (like zooming, shearing, and flipping) to make the model more robust.\n",
    "- We'll create two generators:\n",
    "  - **Training Generator:** Applies augmentation and rescaling.\n",
    "  - **Validation Generator:** Only applies rescaling (no augmentation).\n",
    "- Finally, we'll verify that the generators are working by printing their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preprocessing and Generators\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Validation generator (no augmentation)\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Create training and validation generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {validation_generator.samples}\")\n",
    "print(f\"Class indices: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Building Deep Learning Models\n",
    "\n",
    "We'll implement multiple approaches to compare their effectiveness:\n",
    "\n",
    "1. **Custom CNN**: A convolutional neural network built from scratch\n",
    "2. **Transfer Learning with VGG16**: Using pre-trained weights from ImageNet\n",
    "\n",
    "Let's start with building our custom CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build Custom CNN Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_custom_cnn():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and display the custom CNN\n",
    "custom_model = create_custom_cnn()\n",
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning with VGG16\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "def create_vgg16_model():\n",
    "    # Load pre-trained VGG16 model\n",
    "    base_model = VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom classification head\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create VGG16 model\n",
    "vgg16_model = create_vgg16_model()\n",
    "print(f\"VGG16 model created with {vgg16_model.count_params()} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Training\n",
    "\n",
    "Now we'll train both models and compare their performance. We'll use callbacks for early stopping and learning rate reduction to optimize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup with callbacks\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]\n",
    "\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Custom CNN\n",
    "print(\"Training Custom CNN...\")\n",
    "\n",
    "history_custom = custom_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Custom CNN training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VGG16 Transfer Learning Model\n",
    "print(\"Training VGG16 Transfer Learning Model...\")\n",
    "\n",
    "history_vgg16 = vgg16_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"VGG16 training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation and Comparison\n",
    "\n",
    "Let's evaluate both models and visualize their training progress and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "\n",
    "def plot_training_history(history, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title(f'{title} - Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title(f'{title} - Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training histories\n",
    "plot_training_history(history_custom, 'Custom CNN')\n",
    "plot_training_history(history_vgg16, 'VGG16 Transfer Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "# Evaluate Custom CNN\n",
    "custom_loss, custom_acc = custom_model.evaluate(validation_generator, verbose=0)\n",
    "print(f\"Custom CNN - Validation Accuracy: {custom_acc:.4f}\")\n",
    "print(f\"Custom CNN - Validation Loss: {custom_loss:.4f}\")\n",
    "\n",
    "# Evaluate VGG16\n",
    "vgg16_loss, vgg16_acc = vgg16_model.evaluate(validation_generator, verbose=0)\n",
    "print(f\"VGG16 - Validation Accuracy: {vgg16_acc:.4f}\")\n",
    "print(f\"VGG16 - Validation Loss: {vgg16_loss:.4f}\")\n",
    "\n",
    "# Compare models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Custom CNN Accuracy: {custom_acc:.4f} ({custom_acc*100:.2f}%)\")\n",
    "print(f\"VGG16 Accuracy: {vgg16_acc:.4f} ({vgg16_acc*100:.2f}%)\")\n",
    "\n",
    "if vgg16_acc > custom_acc:\n",
    "    print(f\"\\nðŸ† Winner: VGG16 Transfer Learning\")\n",
    "    print(f\"Performance gain: {((vgg16_acc - custom_acc) * 100):.2f} percentage points\")\n",
    "else:\n",
    "    print(f\"\\nðŸ† Winner: Custom CNN\")\n",
    "    print(f\"Performance gain: {((custom_acc - vgg16_acc) * 100):.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Model Predictions and Visualizations\n",
    "\n",
    "Let's visualize some predictions to understand how well our models are performing on individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "\n",
    "def visualize_predictions(model, generator, model_name, num_images=8):\n",
    "    # Get a batch of images\n",
    "    batch_images, batch_labels = next(generator)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(batch_images)\n",
    "    predicted_classes = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    class_names = ['Parasitized', 'Uninfected']\n",
    "    \n",
    "    for i in range(min(num_images, len(batch_images))):\n",
    "        axes[i].imshow(batch_images[i])\n",
    "        \n",
    "        true_label = int(batch_labels[i])\n",
    "        pred_label = predicted_classes[i][0]\n",
    "        confidence = predictions[i][0]\n",
    "        \n",
    "        # Color coding: green for correct, red for incorrect\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        title = f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\\nConf: {confidence:.3f}'\n",
    "        axes[i].set_title(title, color=color, fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Sample Predictions', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show predictions from both models\n",
    "visualize_predictions(custom_model, validation_generator, 'Custom CNN')\n",
    "visualize_predictions(vgg16_model, validation_generator, 'VGG16 Transfer Learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future Work\n",
    "\n",
    "### Key Achievements:\n",
    "\n",
    "âœ… **Successfully implemented** deep learning models for malaria cell detection\n",
    "\n",
    "âœ… **Compared multiple approaches** - Custom CNN vs Transfer Learning with VGG16\n",
    "\n",
    "âœ… **Applied proper data preprocessing** with augmentation techniques\n",
    "\n",
    "âœ… **Achieved high accuracy** in distinguishing parasitized from uninfected cells\n",
    "\n",
    "### Medical Impact:\n",
    "\n",
    "- **Faster diagnosis** - Automated detection reduces analysis time\n",
    "- **Reduced human error** - Consistent, objective analysis\n",
    "- **Scalable solution** - Can be deployed in resource-limited settings\n",
    "- **Early detection** - Enables prompt treatment initiation\n",
    "\n",
    "### Future Enhancements:\n",
    "\n",
    "1. **Real-time detection system** for live microscopy\n",
    "2. **Mobile application** for field deployment\n",
    "3. **Multi-species detection** to identify different Plasmodium species\n",
    "4. **Integration with lab systems** for automated workflows\n",
    "5. **Clinical validation** studies in healthcare settings\n",
    "\n",
    "This project demonstrates the potential of AI in healthcare, specifically in making malaria diagnosis more accessible, accurate, and efficient worldwide."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
